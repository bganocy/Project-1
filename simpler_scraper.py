# -*- coding: utf-8 -*-
"""Simpler Scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FGBrazLaVzn2JQ_v-bUgJYb5EgVK83Ws
"""

# Install required packages (uncomment if needed):
# pip install requests requests-cache beautifulsoup4 pandas lxml

import re
import time
import json
from urllib.parse import urljoin, urlparse

import requests
import requests_cache
from bs4 import BeautifulSoup
import pandas as pd

BASE = "https://gostanford.com"        # Stanford Athletics main site
USER_AGENT = "ColabStudentScraper/1.0 (respectful; email: you@example.com)"
DELAY_SEC = 1.5                         # polite delay between requests
TIMEOUT = 20

# Years you care about (roster year often appears somewhere on page text/URL)
YEARS = list(range(2025, 2026))         # edit as you like

# If you only care about certain sports, put partial URL path fragments here to filter.
SPORT_FILTERS = []  # e.g., ["/sports/womens-basketball", "/sports/football"]

# Cache requests so repeated runs are faster and kinder to the site
requests_cache.install_cache("stanford_cache", expire_after=60*60*24)  # 1 day

session = requests.Session()
session.headers.update({"User-Agent": USER_AGENT})

def can_fetch_robots(url, robots_txt):
    """
    Very simple robots check: disallow if any Disallow rule matches the path.
    (Not a full parser, but good enough for beginners.)
    """
    try:
        parsed = urlparse(url)
        path = parsed.path or "/"
        for line in robots_txt.splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if line.lower().startswith("disallow:"):
                rule = line.split(":", 1)[1].strip()
                if rule == "":
                    # empty disallow = everything allowed
                    continue
                # naive path check
                if path.startswith(rule):
                    return False
        return True
    except:
        return True  # fail-open if parsing error

def fetch(url):
    """Polite fetch with delay and timeout, returns BeautifulSoup or None."""
    # Robots
    try:
        robots = session.get(urljoin(BASE, "/robots.txt"), timeout=TIMEOUT)
        robots_txt = robots.text if robots.ok else ""
        if not can_fetch_robots(url, robots_txt):
            print(f"robots.txt disallows: {url}")
            return None
    except Exception as e:
        print("robots check error:", e)

    # Fetch
    try:
        r = session.get(url, timeout=TIMEOUT)
        if not getattr(r, "from_cache", False):
            time.sleep(DELAY_SEC)
        if r.ok:
            return BeautifulSoup(r.text, "lxml")
        else:
            print("HTTP error", r.status_code, "for", url)
            return None
    except Exception as e:
        print("Fetch error:", e, "for", url)
        return None

def looks_like_roster_url(href: str, classes: list[str]) -> bool:
    if not href:
        return False
    href = href.lower()
    # Heuristics that often catch roster pages across college sites
    if ("roster" in href) or ("/sports/" in href and ("/players" in href or "/team" in href)):
        return True
    # Also check for "roster" in class names or href
    if classes and any("roster" in c.lower() for c in classes):
        return True
    # Add a check for 'roster' or 'schedule' in the href for broader matching
    if "roster" in href or "schedule" in href:
        return True
    return False


def allowed_by_filters(url: str) -> bool:
    if not SPORT_FILTERS:
        return True
    return any(fragment in url for fragment in SPORT_FILTERS)

def find_roster_links(start_url: str, max_pages: int = 40):
    """
    Crawl from a hub page (like /sports) and collect likely roster page links.
    Keep it small & polite: limit total pages.
    """
    to_visit = [start_url]
    seen = set()
    roster_links = set()

    while to_visit and len(seen) < max_pages:
        url = to_visit.pop(0)
        if url in seen:
            continue
        seen.add(url)

        soup = fetch(url)
        if not soup:
            continue

        for a in soup.select("a[href]"):
            href = a.get("href")
            classes = a.get("class", [])
            if not href:
                continue
            full = urljoin(url, href)
            # stay on same domain
            if urlparse(full).netloc != urlparse(BASE).netloc:
                continue
            # keep crawl small: only queue more /sports/ pages and collect roster-ish links
            if "/sports/" in full and full not in seen and len(seen) + len(to_visit) < max_pages:
                to_visit.append(full)

            # check if the link looks like a roster URL and is allowed by filters
            if looks_like_roster_url(full, classes) and allowed_by_filters(full):
                roster_links.add(full)


    return sorted(roster_links)

def extract_year_from_url(url: str) -> int:
    """
    Extract year from URL patterns:
    - /sports/{sport}/roster/season/{year}
    - /sports/{sport}/roster/{year}-{year+1}
    """
    try:
        # Try to extract from season pattern first (most specific)
        season_match = re.search(r"/season/(\d{4})", url)
        if season_match:
            return int(season_match.group(1))
        # Try winter format: /roster/2025-26
        winter_match = re.search(r"/roster/(\d{4})-\d{2}", url)
        if winter_match:
            return int(winter_match.group(1))
        # Fallback: find any 4-digit year (19xx or 20xx) in URL
        full_match = re.search(r"\b(19\d{2}|20\d{2})\b", url)
        if full_match:
            return int(full_match.group(1))
    except (ValueError, AttributeError):
        pass
    # Default to first year in YEARS list if extraction fails
    return YEARS[0] if YEARS else None

def clean_name(name: str):
    name = (name or "").strip()
    # Remove extra whitespace and weird characters
    name = re.sub(r"\s+", " ", name)
    # Further cleaning: remove common irrelevant phrases
    name = re.sub(r"(?i)Skip To Main Content|Tickets for .*?|Schedule for .*?|Roster for .*?|News for .*?|instagram for .*?|twitter for .*?|facebook for .*?|Opens in a new window", "", name)
    return name

def split_last_name(full_name: str):
    # naive split: last token is last name
    parts = (full_name or "").strip().split()
    if not parts:
        return "", ""
    if len(parts) == 1:
        return parts[0], parts[0]
    first = " ".join(parts[:-1])
    last  = parts[-1]
    return first, last

def extract_roster_entries(url: str):
    soup = fetch(url)
    if not soup:
        return []

    # sport name guess from breadcrumb or title
    title_txt = (soup.title.string if soup.title else "") or ""
    h1_txt = (soup.find("h1").get_text(" ", strip=True) if soup.find("h1") else "")
    sport_guess = ""
    for t in [h1_txt, title_txt]:
        if "roster" in (t or "").lower():
            sport_guess = t
            break
    if not sport_guess:
        # fallback to URL segment
        sport_guess = "/".join(urlparse(url).path.split("/")[:3])

    # Extract year from URL
    year_guess = extract_year_from_url(url)

    # Attempt to extract player names from JSON script tag
    rows = []
    script_tag = soup.find("script", {"type": "application/json"})
    if script_tag:
        try:
            json_data = json.loads(script_tag.string)
            
            # Nuxt.js uses a normalized data format where the ENTIRE JSON array is the string table
            # Integer references like first_name: 255 mean "look at index 255 in this array"
            # The array contains mixed types: strings, dicts, lists, etc.
            
            # If json_data is a list, it IS the string table (each index can contain the referenced value)
            # If json_data is a dict, we need to find the array within it
            string_table = None
            
            if isinstance(json_data, list):
                # The entire array is the string table - integer references are indices into this array
                string_table = json_data
            elif isinstance(json_data, dict):
                # Look for a large list in the dict structure
                def find_array_in_dict(obj, depth=0, max_depth=5):
                    if depth > max_depth:
                        return None
                    if isinstance(obj, list) and len(obj) > 100:
                        return obj
                    elif isinstance(obj, dict):
                        for value in obj.values():
                            result = find_array_in_dict(value, depth+1, max_depth)
                            if result:
                                return result
                    return None
                string_table = find_array_in_dict(json_data)
            
            def resolve_string(value, string_table_ref):
                """Resolve a value - if it's an integer, use it as an index into the string table"""
                if isinstance(value, int) and string_table_ref and 0 <= value < len(string_table_ref):
                    resolved = string_table_ref[value]
                    # If the resolved value is a string, return it
                    if isinstance(resolved, str):
                        return resolved
                    # Otherwise return the string representation
                    return str(resolved) if resolved is not None else ""
                elif isinstance(value, str):
                    return value
                else:
                    return str(value) if value is not None else ""
            
            # Now search for player data (dicts with first_name and last_name keys)
            def find_player_data_simple(data, string_table_ref=None):
                if isinstance(data, dict):
                    if "first_name" in data and "last_name" in data:
                        # Resolve the integer references to actual strings
                        first_name = resolve_string(data["first_name"], string_table_ref)
                        last_name = resolve_string(data["last_name"], string_table_ref)
                        
                        # Only add if we got actual string names (contain letters, not just numbers)
                        if (first_name and last_name and 
                            isinstance(first_name, str) and isinstance(last_name, str) and
                            not first_name.isdigit() and not last_name.isdigit() and
                            re.search(r'[a-zA-Z]', first_name) and re.search(r'[a-zA-Z]', last_name) and
                            len(first_name.strip()) > 0 and len(last_name.strip()) > 0):
                            full_name = f"{first_name.strip()} {last_name.strip()}".strip()
                            rows.append({
                                "source_url": url,
                                "sport": sport_guess,
                                "year": year_guess,
                                "full_name": full_name,
                                "first_name": first_name.strip(),
                                "last_name": last_name.strip(),
                                "last_name_norm": re.sub(r"[^a-z]", "", last_name.strip().lower())
                            })
                    # Recursively search through all values
                    for value in data.values():
                        find_player_data_simple(value, string_table_ref)
                elif isinstance(data, list):
                    for item in data:
                        find_player_data_simple(item, string_table_ref)

            find_player_data_simple(json_data, string_table)
            
            # Remove duplicates based on source_url, first_name, and last_name
            seen_players = set()
            unique_rows = []
            for row in rows:
                player_key = (row["source_url"], row["first_name"].lower(), row["last_name"].lower())
                if player_key not in seen_players:
                    seen_players.add(player_key)
                    unique_rows.append(row)
            rows = unique_rows

        except json.JSONDecodeError:
            print("Could not decode JSON from script tag.")
        except Exception as e:
            print("Error processing JSON data:", e)
            import traceback
            traceback.print_exc()

    # No fallback to heuristics in this simplified version
    return rows

# OPTIONAL: Discovery crawl (commented out - using direct URL generation instead)
# Uncomment below if you want to discover roster pages by crawling
# sports_hub = urljoin(BASE, "/sports/")
# discovered_roster_pages = find_roster_links(sports_hub, max_pages=40)
# print(f"Found {len(discovered_roster_pages)} likely roster pages (showing a few):")
# for u in discovered_roster_pages[:8]:
#     print(" -", u)

# FALL SPORTS - their season is this year (e.g. now is 2025)
sport_names_fall = [
    "field-hockey",
    #"volleyball"
]

#generate fall roster URLS to scrape
all_roster_urls = []
fall_roster_urls = []
for sport in sport_names_fall:
  specific_sport_roster_urls = []
  for year in YEARS:
      specific_sport_roster_urls.append(urljoin(BASE, f"/sports/{sport}/roster/season/{year}")) #checks every year
      fall_roster_urls.append(urljoin(BASE, f"/sports/{sport}/roster/season/{year}"))
  print(f"{sport} - FALL - generated {len(specific_sport_roster_urls)} potential roster URLs.")
  print(f"Showing All for {sport}")
  for u in specific_sport_roster_urls[:23]:
      print(" -", u)
print(f"FALL SPORTS - generated {len(fall_roster_urls)} potential roster URLs.")
all_roster_urls.extend(fall_roster_urls)
print(f"TOTAl = {len(all_roster_urls)} so far")

# WINTER SPORTS - their season is between years (e.g. rn is 2025-26)
sport_names_winter = [
    #"womens-basketball"
]

#generate winter roster URLS to scrape
winter_roster_urls = []
for sport in sport_names_winter:
  specific_sport_roster_urls = []
  for year in YEARS:
      specific_sport_roster_urls.append(urljoin(BASE, f"/sports/{sport}/roster/{year}-{str(year+1)[2:]}"))
      winter_roster_urls.append(urljoin(BASE, f"/sports/{sport}/roster/{year}-{str(year+1)[2:]}"))
  print(f"{sport} - WINTER - generated {len(specific_sport_roster_urls)} potential roster URLs.")
  print(f"Showing All for {sport}")
  for u in specific_sport_roster_urls[:23]:
      print(" -", u)
print(f"WINTER SPORTS - generated {len(winter_roster_urls)} potential roster URLs.")
all_roster_urls.extend(winter_roster_urls)
print(f"TOTAl = {len(all_roster_urls)} so far")


#SPRING SPORTS - their season is the year after (e.g. rn is 2026)
sport_names_spring = [
    #"artistic-swimming"
]

#generate spring roster URLS to scrape
spring_roster_urls = []
for sport in sport_names_spring:
  specific_sport_roster_urls = []
  for year in YEARS:
      specific_sport_roster_urls.append(urljoin(BASE, f"/sports/{sport}/roster/season/{year+1}")) #checks every year
      spring_roster_urls.append(urljoin(BASE, f"/sports/{sport}/roster/season/{year+1}")) #checks every year
  print(f"{sport} - SPRING - generated {len(specific_sport_roster_urls)} potential roster URLs.")
  print(f"Showing All for {sport}")
  for u in specific_sport_roster_urls[:23]:
      print(" -", u)
print(f"SPRING SPORTS - generated {len(spring_roster_urls)} potential roster URLs.")
all_roster_urls.extend(spring_roster_urls)
print(f"TOTAL = {len(all_roster_urls)} roster URLs generated")

# Use the generated list of roster URLs
roster_pages = all_roster_urls

print(f"\nAttempting to fetch from {len(roster_pages)} potential roster URLs.\n")

# Extract athlete rows from all roster pages
all_rows = []
for i, url in enumerate(roster_pages, start=1):
    print(f"[{i}/{len(roster_pages)}] parsing: {url}")
    try:
        rows = extract_roster_entries(url)
        all_rows.extend(rows)
        if rows:
            print(f"  Found {len(rows)} players")
    except Exception as e:
        print(f"  Parse error: {e}")

# Create DataFrame and display results
df = pd.DataFrame(all_rows)
print(f"\n{'='*60}")
print(f"Total rows extracted: {len(df)}")
print(f"{'='*60}")

if len(df) > 0:
    print("\nFirst 10 rows:")
    print(df.head(10).to_string())
    print(f"\nDataFrame shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")
else:
    print("\nNo data extracted. Check URLs and JSON structure.")

